**以下是今天组会我展示部分的所有论文列表：**

[1] [Paras Jain, Zhanghao Wu, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. *Advances in Neural Information Processing Systems*, 34, 2021.](https://proceedings.neurips.cc/paper/2021/file/6e67691b60ed3e4a55935261314dd534-Paper.pdf)

[2] [Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. *arXiv preprint arXiv:2007.02835*, 2020.](https://proceedings.neurips.cc/paper/2020/file/94aef38441efa3380a3bed3faf1f9d5d-Paper.pdf)

[3] [Gr´egoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. *arXiv preprint arXiv:2106.05667*, 2021.](https://arxiv.org/pdf/2106.05667.pdf)

[4] [Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh graphormer. *arXiv preprint arXiv:2104.00272*,2021.](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Mesh_Graphormer_ICCV_2021_paper.pdf)

[5] [Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for learning graph representations. *arXiv preprint arXiv:2001.05140*, 2020.](https://arxiv.org/pdf/2001.05140.pdf)

[6] [Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. *arXiv preprint arXiv:2012.09699*, 2020.](https://arxiv.org/pdf/2012.09699.pdf)

[7] [Md Shamim Hussain, Mohammed Zaki, and Dharmashankar Subramanian. Edge-augmented graph transformers: Global self-attention is enough for graphs. *arXiv preprint arXiv:2108.03348*, 2021.](https://arxiv.org/pdf/2108.03348.pdf)

[8] [Shaowei Yao, Tianming Wang, and Xiaojun Wan. Heterogeneous graph transformer for graph-to-sequence learning. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7145–7154, 2020.](https://aclanthology.org/2020.acl-main.640.pdf)

[9] [Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pages 7464–7471, 2020.](https://ojs.aaai.org/index.php/AAAI/article/view/6243)

[10] Devin Kreuzer, Dominique Beaini, William L Hamilton, Vincent L´etourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. *arXiv preprint arXiv:2106.03893*, 2021.

[11] [Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Peilin Zhao, Junzhou Huang, Da Luo, Kangyi Lin, and Sophia Ananiadou. Masked transformer for neighhorhood-aware click-through rate prediction. *arXiv preprint arXiv:2201.13311*, 2022.](https://arxiv.org/pdf/2201.13311.pdf)

[12] [Jianan Zhao, Chaozhuo Li, Qianlong Wen, Yiqi Wang, Yuming Liu, Hao Sun, Xing Xie, and Yanfang Ye. Gophormer: Ego-graph transformer for node classification. *arXiv preprint arXiv:2110.13094*, 2021.](https://arxiv.org/pdf/2110.13094.pdf)

[13] Ling Min Serena Khoo, Hai Leong Chieu, Zhong Qian, and Jing Jiang. Interpretable rumor detection in microblogs by attending to user interactions. In *Proceedings of the AAAI Conference on Artifificial Intelligence*, volume 34, pages 8783–8790, 2020.

[14] Kuang, Weirui, et al. "Coarformer: Transformer for large graph via graph coarsening." (2021).

[15] [Park, Jinyoung, et al. "Deformable Graph Transformer." *arXiv preprint arXiv:2206.14337*
 (2022).](https://arxiv.org/pdf/2206.14337.pdf)